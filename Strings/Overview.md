Table of current best bounds for various problems (to the best of my knowledge and at the time of this writing):
| **Result Type** | **Result** | **Paper** | **Date of Result** |
|:---------------:|:----------:|:---------:|:------------------:|
| (Upper Bound) Arbitrary String Trace Reconstruction | $\exp\left(O(n^{1/5} \log^{5}(n))\right)$ | New Upper Bounds For Trace Reconstruction |         2020 |
| (Lower Bound) Worst Case String Trace Reconstruction | $\Omega\left(\frac{n^{3/2}}{\log^7(n)}\right)$ | New Lower Bounds for Trace Reconstruction | 2019 |
| (Lower Bound) Average Case String Trace Reconstruction  |  $\Omega\left(\frac{\log^{5/2}(n)}{(\ \log(\log(n))\ )^7}\right)$  |  New Lower Bounds for Trace Reconstruction  | 2019 |
| (Upper Bound) Random String Trace Reconstruction | $\exp\left(O(\log^{1/3}(n))\right)$  | Subpolynomial trace reconstruction for random strings and arbitrary deletion probability  | 2018  |
| (Upper Bound) Coded Trace Reconstruction  | Rate $1 - \frac{1}{\log(n)}$, recoverable from $\exp\left(O(\log^{2/3}(n))\right)$ traces   | Coded Trace Reconstruction | 2020 |
| (Upper Bound) Approximate Trace Reconstruction  | For retention rate $\rho= \omega(\log(n) / n)$, can reconstruct from 1 trace, with $\mathbb{E}$[LCS] $\geq 2/3 - o(1) n$ |  Approximate Trace Reconstruction from a Single Trace  | 2023 |



LCS = longest common subsequence (meaning the longest section of the recovered string that agrees with original string). 


# Reconstructing Strings From Random Traces
\cite{batu2004reconstructing} by Tugkan Batu, Sampath Kannan, Sanjeev Khanna, and Andrew McGregor. This paper introduces the problem of string trace reconstruction. 
    
The motivation is the ``sequence-alignment problem'' from biology, which is the problem of aligning a sequence (or string) of DNA characters, given that the sequences underwent mutations which can add, delete or replace elements of the sequence. They assume a simplified version of the problem, where only deletions are allowed as sequence mutations. 

Formally, their problem statement is: given a string $t$ of length $n$, each trace $\tilde{t}$ (using my own notation for this) is a generated by taking each element of the string and deleting it with probability $p$. They ask, how many traces, $m$, are required to fully reconstruct the string $t$ with high probability?

To solve this problem they use an algorithm they call ``Bitwise Majority Alignment'', which attempts to find an index-based alignment, so that the majority of the traces will be in agreement on what the next element of the string is. More clearly, for each trace $\tilde{t}_1$, $\dots$, $\tilde{t}_m$, they look at an index variable that will point to what the trace thinks should be the next element of the string. Then, by using a majority voting scheme, the algorithm decides what the next value of the string should be, and those traces that agreed with the vote get moved up an index, and those that did not agree with the vote will not have their index incremented. The idea is that if an element was deleted from a trace then when the algorithm gets to the string index that was deleted from the trace, it will hold the trace back one step when it disagrees with the majority (and the majority will not have that element deleted by averaging), it will be held back one step because of its disagreement, which will bring it in alignment with the actual values of the original string $t$. 

The paper provides these guarantees: For the vast majority of strings, and $p = O(\frac{1}{\log(n)})$, they can reconstruct the string with only $m = O(
\log(n))$ traces. They also show that $m=\Omega(\frac{\log(n)}{\log(\log(n))})$ strings are necessary to recover the original string. As another result, they show that if $p = \Omega(1 / n^{1/2 + \varepsilon})$ they can recover a string very close to the original string with only $m = O(\frac{1}{\varepsilon})$ traces. 


# Trace Reconstruction: Generalized and Parameterized
\cite{krishnamurthy2021trace} by Akshay Krishnamurthy, Arya Mazumdar, Andrew McGregor, and Soumyabrata Pal. This paper covers several versions of the string trace reconstruction problem. In particular they deal with string trace reconstruction for $k$-sparse strings, and a matrix trace reconstruction problem for worst case and randomly generated matrices. In this paper, they have $p$ being the deletion probability (though they also deal with specific variants involving different deletion probabilities for $1$'s and $0$'s), $n$ the length of the string or total number of elements in the matrix, and $m$ being the number of traces required. 

They employ four different techniques for proving all of these results: learning the parameters of a binomial mixture distribution, hierarchical clustering, a k-deck combinatorial argument, and a multivariate polynomial complex analysis technique (similar to previous works with evaluating Littlewood polynomials). 

Their results are as follows: 
- *Theorem 1*: For a string $x \in (0,1)^n$, with at most $k$ non-zero elements, then assuming $1 - p = \Omega( k^{-1/2} \log(n) )$, $m = \exp\{ O( (k/(1-p))^{1/3} \log^{2/3}(n) ) \}$ traces will allow recovery with high probability.
- *Theorem 2*: Again, $k$ being the sparsity of the string. If each $1$ is separated by at least $\Omega(k \log(n) )$ 0's, then for any $p$ poly $(n)$ traces suffice.
- *Theorem 3*: For any $x,y \in \{0,1\}$ with Hamming distance $d_H(x,y) \leq 2k$, and if $p \leq 1 - k/n$, then $\exp\{O(k \log(n))\}$ traces are enough to distinguish between $x$ and $y$.
- *Theorem 4*: They define a matrix deletion for a $\sqrt{n} \times \sqrt{n}$-dimensional matrix to be where you delete row and column $i$ with probability $p$ (both row and column are deleted at the same time). They show $\exp\{O(n^{1/4}/(1-p) \sqrt{p \log(n)})\}$ traces are sufficient to recover the matrix from a matrix deletion channel, with high probability.
- *Theorem 5*: Let $X \in ( 0,1 )^{\sqrt{n} \times \sqrt{n}}$ be such that each element has distribution Bern(1/2). Then $O(\log(n))$ traces suffice to recover $X$ with high probability.


To prove Theorem 1, they show a stronger result involving an Austere Deletion Channel (ADC). An Austere Deletion Channel is a channel that deletes all but one "0", where the "0" not deleted is chosen uniformly at random. They connect this to the standard deletion channel through simple probabilistic arguments. They then show that essentially the same number of traces are required to reconstruct from the ADC as the claim in Theorem 1. They do this by thinking of $x$ as a multiset $(r_1, \dots, r_{n_0})$, where $r_i$ is the number of 1s before the $i$th 0 appears in the string. Then, they show that by taking $s_k$ to be the number of 1s in front of the only 0 in the $k$th sample from an ADC, they can write the distribution of $s_k$ as a mixture of Binomial distributions, where each Binomial distribution in the mixture is Bin($r_i$, $1 - p_1$) [ $p_1$ is the probability of deleting 1s, which is just $p$ in Theorem 1 ]. Finally, they do a little bit more manipulation, and then apply a technique similar to \cite{nazarov2017trace} ("Trace Reconstruction with $\exp(O(n)^{1/3}$ samples"), to show the sample complexity of learning the $r_i$'s from $s_1, \dots, s_m$, which is essentially the same sample complexity as they claim in Theorem 1. 

To prove Theorem 2 they construct a graph for each trace on $k$ vertices. Each vertex in the graph is given a visible vertex label, $z_v$, that corresponds to the position of a corresponding 1 in the received trace, and an invisible label, $y_v$, that corresponds to the order (among 1's) that the associated 1 had in the original string. They construct edges between vertices $v$ and $w$ if $|z_v - z_w| \leq 2 c\sqrt{n \log(n)}$, where the threshold comes from a Chernoff concentration result. They then perform recursion, similar to a hierarchical clustering algorithm, to get the weaker bound they require in Theorem 2. 


# Optimal Mean-Based Algorithms for Trace Reconstruction 
\cite{de2017optimal} by Anindya De, Ryan O’Donnell, Rocco A. Servedio. This paper provides a very clean solution that gave the state of the art for worst case string trace reconstruction, where they required $m = \exp(O(n)^{1/3})$ traces. _This was eventually improved in \cite{chase2020new}, where they improved the result to $m = \exp(O(n)^{1/5})$ using different polynomials and other techniques_. 

Ryan O'Donnell explains the paper far better than I could in [this YouTube video](https://www.youtube.com/watch?v=Ys11H5smSIM). 

This paper was a serious step forward in string trace reconstruction, specifically because it showed an equivalence between the string trace reconstruction problem, and finding Littlewood polynomials on an disk in the complex plane with the smallest modulus. They first showed that the worst case string trace reconstruction problem can be made equivlanent to finding a polynomial with small modulus on the unit disk in the complex plane, and then reduced that problem through probabilistic analysis to the Littlewood Polynomial analysis problem. 

The paper also showed results for a general deletion, insertion, bit flip channel, though the analysis process was largely the same as the deletion channel. The sample complexity was relatively similar to the sample complexity of recovering from the deletion channel. They also found an interesting result, that sometimes more insertions can help with string trace recovery in very specific settings. 



# New Upper Bounds for Trace Reconstruction
\cite{chase2020newU} by Zachary Chase. This paper improves on the previous $m = \exp(O(n)^{1/3})$ bound for worst case string trace reconstruction, improving it to $m = \exp(\tilde{O}(n)^{1/5})$ instead. 

The main result of this paper is: 
- *Theorem 1*: For any deletion probability $q \in (0,1)$ and any $\delta > 0$, there exists $C > 0$ so that any unknown string $x \in \{0,1\}^n$ can be reconstructed with probability at least $> 1 - \delta$ from $T = \exp(Cn \log^5(n))$ i.i.d. traces of $x$.

This is the current state of the art result for trace reconstruction of arbitrary binary strings. The authors adapt the technique used in \cite{de2017optimal} and \cite{nazarov2017trace}. Instead of looking at polynomials who's coefficients are described $\sum_{k=1}^n [x_k -y_k]z^k$, i.e. polynomials with coefficients described by the difference in bit values between the elements of a trace $y$ and the original string $x$, this paper instead used polynomials described by $\sum_{k=1}^n [1_{x_{k+i} = w_i} - 1_{y_{k+i} = w_i}] z^k$, which measures whether a substring of a trace $y$ agrees with the original string $x$. This uses the shorthand $1_{x_{k+i} = w_i} = \prod_{i=1}^{|w|-1} 1_{x_{k+i} = w_i}$ which asks if the indices starting from $x_{k}$ and going to $x_{k+|w|-1}$ agree with the substring $w$. 

They specifically choose the $w$'s that they test with so that these polynomials will be sparser near 1 on the real axis, which combined with the authors result on the separating words problem \cite{chase2020new} along with the results in \cite{borwein1997littlewood} to get an even tighter upper bound on the number of required traces for this problem. 


# Coded Trace Reconstruction 
\cite{cheraghchi2020coded} by Mahdi Cherargchi, Ryan Gabrys, Olgica Milenkovic, João Ribeiro. 

${\textsf{\color{red}TODO: }}$ **Review this paper next**


